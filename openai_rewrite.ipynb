{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scratchpad for the LangChain tutorial demos to compare/contrast LangChain and OpenAI implementations.\n",
    "\n",
    "Note that the documentation was made with `gpt-3.5-turbo-0301`; this notebook will use the updated `gpt-3.5-turbo-0613` for posterity.\n",
    "\n",
    "Time is calculated for each cell to attempt to measure LangChain overhead, but results are variable so analysis is not performed. Times kept for posterity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello World\n",
    "\n",
    "The output of the Hello World is slightly different compared to the documentation due to the updated model (it's more accurate!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 149 ms, total: 1.21 s\n",
      "Wall time: 2.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "chat.predict_messages([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.46 ms, sys: 1.16 ms, total: 4.62 ms\n",
      "Wall time: 672 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"J'adore la programmation.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import openai\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Translate this sentence from English to French. I love programming.\"}]\n",
    "\n",
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages, temperature=0)\n",
    "response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.24 ms, sys: 1.29 ms, total: 6.54 ms\n",
      "Wall time: 961 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"The following is a friendly conversation between a human and an AI. The AI is talkative and \"\n",
    "        \"provides lots of specific details from its context. If the AI does not know the answer to a \"\n",
    "        \"question, it truthfully says it does not know.\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "CPU times: user 3.62 ms, sys: 1.63 ms, total: 5.25 ms\n",
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import openai\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\":\n",
    "        \"The following is a friendly conversation between a human and an AI. The AI is talkative and \"\n",
    "        \"provides lots of specific details from its context. If the AI does not know the answer to a \"\n",
    "        \"question, it truthfully says it does not know.\"}]\n",
    "\n",
    "user_message = \"Hi there!\"\n",
    "messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages, temperature=0)\n",
    "assistant_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "print(assistant_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
